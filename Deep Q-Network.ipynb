{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from Environment import Tetris\n",
    "from pygame.time import Clock\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Tetris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_shape = env.observation_shape()\n",
    "n_inputs = observation_shape[0]*observation_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = env.action_scope_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(512, activation='relu', input_shape=[n_inputs]),\n",
    "                          keras.layers.Dense(256, activation='relu'),\n",
    "                          keras.layers.Dense(n_outputs)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(obs.reshape(1, -1))\n",
    "    \n",
    "    return np.argmax(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = pd.read_csv('./checkpoints/dataset_tetris.csv')\n",
    "\n",
    "    dataset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "    state_string = dataset['state'].to_numpy()\n",
    "    action = dataset['action'].to_numpy()\n",
    "    reward = dataset['reward'].to_numpy()\n",
    "    next_state_string = dataset['next_state'].to_numpy()\n",
    "    done = dataset['done'].to_numpy()\n",
    "\n",
    "    state = []\n",
    "    next_state = []\n",
    "\n",
    "    for i in range(len(state_string)):\n",
    "        list_a = []\n",
    "        list_b = []\n",
    "        for j in range(len(state_string[i])):\n",
    "            list_a.append(int(state_string[i][j]))\n",
    "            list_b.append(int(next_state_string[i][j]))\n",
    "\n",
    "        state.append(np.array(list_a, dtype=np.float32))\n",
    "        next_state.append(np.array(list_b, dtype=np.float32))\n",
    "    \n",
    "    replay_memory.extend(list(zip(state, action, reward, next_state, done)))\n",
    "except FileNotFoundError:\n",
    "    print('None training memory. Starting from the beginning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state.flatten(), done))\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_episode = 0\n",
    "\n",
    "for episode in range(0, 50000, 1000):\n",
    "    try:\n",
    "        model.load_weights('./checkpoints/' + str(episode) + '_episodes_tetris')\n",
    "        first_episode = episode + 1\n",
    "    except tf.errors.NotFoundError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(episode):\n",
    "    state, action, reward, next_state, done = [[replay_memory[index][field_index] for index in range(len(replay_memory))] for field_index in range(5)]\n",
    "    \n",
    "    state_string = []\n",
    "    next_state_string = []\n",
    "\n",
    "    for i in range(len(state)):\n",
    "        string_a = \"\"\n",
    "        string_b = \"\"\n",
    "        for j in range(200):\n",
    "            string_a += str(int(state[i][j]))\n",
    "            string_b += str(int(next_state[i][j]))\n",
    "        state_string.append(string_a)\n",
    "        next_state_string.append(string_b)\n",
    "        \n",
    "    dataset = pd.DataFrame(columns=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    dataset['state'] = state_string\n",
    "    dataset['action'] = action\n",
    "    dataset['reward'] = reward\n",
    "    dataset['next_state'] = next_state_string\n",
    "    dataset['done'] = done\n",
    "    \n",
    "    dataset.to_csv('./checkpoints/dataset_tetris.csv', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50000, Steps: 96,  Score: 95, Best score: 243, eps: 0.01010"
     ]
    }
   ],
   "source": [
    "for episode in range(first_episode, 50001):\n",
    "    obs = env.reset()\n",
    "    state = obs.flatten()\n",
    "    \n",
    "    score = 0\n",
    "    for step in range(1000):\n",
    "        epsilon = max(1 - episode / 41900, 0.01)\n",
    "        obs, reward, done = play_one_step(env, state, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            score = env.get_score()\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = score\n",
    "    print(\"\\rEpisode: {}, Steps: {},  Score: {}, Best score: {}, eps: {:.3f}\".format(episode, step + 1, score,best_score, epsilon), end=\"\")\n",
    "\n",
    "    if episode > 4190:\n",
    "        training_step(batch_size)\n",
    "        \n",
    "    if episode % 1000 == 0:\n",
    "        model.save_weights('./checkpoints/' + str(episode) + '_episodes_tetris')\n",
    "        save_memory(episode)\n",
    "\n",
    "        \n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clock = Clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     env.render()\n",
    "    \n",
    "#     state = obs.flatten()\n",
    "#     action = epsilon_greedy_policy(state)\n",
    "    \n",
    "#     obs, reward, done = env.step(action)\n",
    "    \n",
    "#     clock.tick(16)\n",
    "    \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./checkpoints/10000_episodes_tetris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
